{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the necessary libraries\n",
    "\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "import google.generativeai as genai\n",
    "import google.ai.generativelanguage as glm\n",
    "from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n",
    "from langchain_google_genai.embeddings import GoogleGenerativeAIEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "import os\n",
    "from langchain_google_genai import GoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use google ai studio to get your api key\n",
    "google_api_key = os.getenv('API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain's way to accessing gemini pro\n",
    "# google_genai = ChatGoogleGenerativeAI(\n",
    "#     google_api_key=google_api_key,\n",
    "#     model='gemini-pro')\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-pro\",\n",
    "    google_api_key=google_api_key,\n",
    "    temperature=0.1,\n",
    "    convert_system_message_to_human=True)\n",
    "\n",
    "# google's way to access gemini pro\n",
    "# google_genai = genai.configure(api_key=google_api_key)\n",
    "# llm = genai.GenerativeModel('gemini-pro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = GoogleGenerativeAI(model=\"gemini-pro\", google_api_key=google_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings to store vectorized texts\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model='models/embedding-001',\n",
    "    google_api_key=google_api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the prompt files\n",
    "loader = CSVLoader(file_path='prompt_and_response.csv', source_column='prompt')\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intialize the vector db\n",
    "vector_db = FAISS.from_documents(documents=data, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the retriver object from vector db\n",
    "retriever = vector_db.as_retriever()\n",
    "# prompt = 'What is the price of this course?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=model, retriever=vector_db.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"Given the following context and a question, generate an answer based on this context only.\n",
    "In the answer try to provide as much text as possible from \"response\" section in the source document context without making much changes.\n",
    "If the answer is not found in the context, kindly state \"I don't know.\" Don't try to make up an answer.\n",
    "\n",
    "CONTEXT: {context}\n",
    "\n",
    "QUESTION: {question}\"\"\"\n",
    "\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(llm=model,\n",
    "                                    chain_type=\"stuff\",\n",
    "                                    retriever=retriever,\n",
    "                                    input_key=\"query\",\n",
    "                                    return_source_documents=True,\n",
    "                                    chain_type_kwargs=chain_type_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample prompt template\n",
    "\n",
    "prompt_template = \"\"\"Given the following context and a question, generate an answer based on this context only.\n",
    "    In the answer try to provide as much text as possible from \"response\" section in the source document context without making much changes.\n",
    "    If the answer is not found in the context, kindly state \"I don't know.\" Don't try to make up an answer.\n",
    "\n",
    "    CONTEXT: {context}\n",
    "\n",
    "    QUESTION: {question}\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
