{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the necessary libraries\n",
    "\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "import google.generativeai as genai\n",
    "import google.ai.generativelanguage as glm\n",
    "from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n",
    "from langchain_google_genai.embeddings import GoogleGenerativeAIEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.vectorstores import FAISS\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use google ai studio to get your api key\n",
    "google_api_key = os.getenv('API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure gemini pro\n",
    "google_genai = ChatGoogleGenerativeAI(\n",
    "    google_api_key=google_api_key,\n",
    "    model='gemini-pro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings to store vectorized texts\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model='models/embedding-001',\n",
    "    google_api_key=google_api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the prompt files\n",
    "loader = CSVLoader(file_path='prompt_and_response.csv', source_column='prompt')\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intialize the vector db\n",
    "vector_db = FAISS.from_documents(documents=data, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the retriver object from vector db\n",
    "retriever = vector_db.as_retriever()\n",
    "relavent_docs = retriever.get_relevant_documents('How about job support?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='prompt: Do you provide any virtual internship?\\nresponse: Yes', metadata={'source': 'Do you provide any virtual internship?', 'row': 14}),\n",
       " Document(page_content='prompt: Do you provide any job assistance?\\nresponse: Yes, We help you with resume and interview preparation along with that we help you in building online credibility, and based on requirements we refer candidates to potential recruiters.', metadata={'source': 'Do you provide any job assistance?', 'row': 11}),\n",
       " Document(page_content='prompt: How can I get help if I have a doubt and need support?\\nresponse: We have an active discord server where you can post your question and most of the time you will get an answer in a reasonable time frame.', metadata={'source': 'How can I get help if I have a doubt and need support?', 'row': 23}),\n",
       " Document(page_content='prompt: Do we have an EMI option?\\nresponse: No', metadata={'source': 'Do we have an EMI option?', 'row': 13})]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample answer from prompt\n",
    "relavent_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample prompt template\n",
    "\n",
    "prompt_template = \"\"\"Given the following context and a question, generate an answer based on this context only.\n",
    "    In the answer try to provide as much text as possible from \"response\" section in the source document context without making much changes.\n",
    "    If the answer is not found in the context, kindly state \"I don't know.\" Don't try to make up an answer.\n",
    "\n",
    "    CONTEXT: {context}\n",
    "\n",
    "    QUESTION: {question}\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
